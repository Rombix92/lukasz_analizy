---
title: "Occupational Employment Statistics"
author: "Lukasz"
date: "11 11 2019"
output: html_document
---

```{r setup, include=FALSE}
url<-('https://assets.datacamp.com/production/repositories/1219/datasets/1e1ec9f146a25d7c71a6f6f0f46c3de7bcefd36c/oes.rds')
download.file(url,"best2.My.Lu2.rds", method="curl")
oes <- readRDS("best2.My.Lu2.rds")
oes <- data.frame(oes)

```

```{r}
#Hierarchical clustering: Occupation trees

library(dendextend)
dist_oes <- dist(oes, method='euclidean')
hc_oes <- hclust(dist_oes, method='average')
dend_oes <- as.dendrogram(hc_oes)
plot(dend_oes)
dend_colored <- color_branches(dend_oes, h=100000)
plot(dend_colored)
```

```{r pressure, echo=FALSE}
#Hierarchical clustering: Preparing for exploration
library(dplyr)
library(tibble)
library(tidyr)
library(ggplot2)
df_oes <- rownames_to_column(as.data.frame(oes), var = 'occupation')
# Create a cluster assignment vector at h = 100,000
cut_oes <- cutree(hc_oes, h = 100000)
df_oes <- mutate(df_oes, cluster=cut_oes)
#creating tidy format for ggplot2
oes_tidy <- gather(df_oes, Year, Salary, -cluster, -occupation)
sort(cut_oes)

oes_tidy %>% ggplot(aes(x=Year,y=Salary, color=factor(cluster))) +
  geom_line(aes(group=occupation))
```

```{r}
#K-means: Elbow analysis
#calculating totall within cluster sum of square distance
#because kmeans consist of random component (when it start it ascribe each observation to n-th group randomly such, each group is at the begining equal in size) it may emerge with different results depending on random choose. Thats why we may say how many time it should the process to be rerun (nstart) and then choose the best solution. It is also good to set.seed at the begining to ensure reproductability of analysis
total_withinss <- map_dbl(1:10, function(k){
  model <- kmeans(x=oes, centers=k, nstart=20)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame( k = 1:10,tot_withinss = total_withinss)
ggplot(elbow_df, aes(x=k, y=total_withinss))+
  geom_line()+
  scale_x_continuous(breaks = 1:10)

#Elbow analysis propose different number of clusters then hierarhical analysis
```

```{r}
#Silhouette widths method 
library(cluster)
sil_width<- map_dbl(2:10, function(k){
  model<-pam(oes, k=k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(k = 2:10,  sil_width = sil_width)

ggplot(sil_df, aes(x=k, y=sil_width))+
  geom_line()

# Here 7 clusters seems as good as 2
```

You ran three different methods for finding the optimal number of clusters and their assignments and you arrived with three different answers.


```{r}
#PCA models in R produce additional diagnostic and output components:

#1)center: the column means used to center to the data, or FALSE if the data weren't centered

#2)scale: the column standard deviations used to scale the data, or FALSE if the data weren't scaled

#3)rotation: the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components

#4)x: the value of each observation in the original dataset projected to the principal components

url='https://assets.datacamp.com/production/course_6430/datasets/Pokemon.csv'
pokemon=read.csv(url)
pokemon=select(pokemon, c("HitPoints","Attack","Defense","Speed"))
# Perform scaled PCA: pr.out
pr.out<-prcomp(pokemon, scale=TRUE)

# Inspect model output
summary(pr.out)

#As stated in the video, the biplot() function plots both the principal components loadings and the mapping of the observations to their first two principal component values.

biplot(pr.out)

#Using the biplot() of the pr.out model, we can see that two original variables have approximately the same loadings in the first two principal components "Attack" and "Hitpoints"
```
```{r}
#The second common plot type for understanding PCA models is a scree plot. A scree plot shows the variance explained as the number of principal components increases.

# Variability of each principal component: pr.var
pr.var <- pr.out$sdev^2

# Variance explained by each principal component: pve
pve <- pr.out$sdev^2 / sum(pr.out$sdev^2)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")


```

CASE STUDY
```{r} 
#Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at

url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1903/datasets/WisconsinCancer.csv"

# Download the data: wisc.df
wisc.df<- read.csv(url)

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector.
#Finally, set a vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.
diagnosis <- as.numeric(wisc.df$diagnosis == 'M')

dim(select(wisc.df, contains('_mean')) )


#Check the mean and standard deviation of the features of the data to determine if the data should be scaled. # Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data, 2, sd)

# Execute PCA, scaling if appropriate: wisc.pr
wisc.pr <- prcomp(wisc.data, scale = TRUE)

# Look at summary of results
summary(wisc.pr)

# Create a biplot of wisc.pr
biplot(wisc.pr)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")

# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")

# Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.

#Variance explained
#In this exercise, you will produce scree plots showing the proportion of variance explained as the number of principal components increases. The data from PCA must be prepared for these plots, as there is not a built-in function in R to create them directly from the PCA model.

# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- wisc.pr$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")


# Scale the wisc.data data: data.scaled
data.scaled<- scale(wisc.data)

# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)

# Create a hierarchical clustering model: wisc.hclust
wisc.hclust<-hclust(data.dist, method='complete')

plot(wisc.hclust)


#In this exercise, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing unsupervised learning like this, a target variable isn't available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model.

#When performing supervised learning—that is, when you're trying to predict some target variable of interest and that target variable is available in the original data—using clustering to create new features may or may not improve the performance of the final model. This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.

# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, h=20)

# Compare cluster membership to actual diagnoses
table(diagnosis, wisc.hclust.clusters)

# Create a k-means model on wisc.data: wisc.km
wisc.km <- kmeans(scale(wisc.data), center=2, nstart=20)

# Compare k-means to actual diagnoses
table(diagnosis,wisc.km$cluster)

# Compare k-means to hierarchical clustering
table(wisc.km$cluster,wisc.hclust.clusters)

```